# 锔 Decisiones T茅cnicas Tomadas en el Desarrollo del ETL

## 1. Infraestructura y Herramientas a Utilizar

Durante la planificaci贸n y construcci贸n del proceso ETL para el dataset de NYC Yellow Taxi, se consideraron varias alternativas basadas en eficiencia, escalabilidad y facilidad de implementaci贸n.

### Opci贸n 1: Servicios Cloud - AWS (Athena + Glue)

- **Idea inicial:** Desarrollar el pipeline utilizando servicios administrados en la nube como **AWS Athena** (para consultas sobre datos en S3) y **AWS Glue** (para automatizar ETL mediante crawlers y jobs Spark).
- **Ventajas esperadas:**
  - Gesti贸n autom谩tica de infraestructura.
  - Escalabilidad bajo demanda.
  - Posibilidad de orquestar pipelines v铆a Glue Workflows.
- **Raz贸n de descarte:**
  - Restricciones de presupuesto.
  - Acceso limitado a las credenciales de los servicios cloud necesarios.
  - Tiempo adicional requerido para la configuraci贸n de IAM roles, S3 buckets y Workflows.

---

### Opci贸n 2: Apache Spark (PySpark)

- **Segunda alternativa:** Construir el pipeline ETL localmente utilizando **PySpark**, como suger铆a el enunciado de la prueba.
- **Pasos realizados:**
  - Configuraci贸n inicial de una **SparkSession** usando `SparkSession.builder`.
  - Intento de correr procesos Spark en local mode (`master("local[*]")`).
- **Problemas encontrados:**
  - **Alto tiempo de inicializaci贸n** de la sesi贸n, incluso para datasets medianos (~700MB).
  - **Consumo elevado de recursos** de CPU y RAM.
  - Creaci贸n de una imagen Docker personalizada para Spark:
    - Inclu铆a Java (OpenJDK 8), Hadoop binaries, PySpark.
    - **Tama帽o total** de la imagen > **15GB**.
    - El proceso de build y ejecuci贸n de contenedores era muy pesado y lento en entornos de hardware moderado.

---

### Opci贸n 3: Python Nativo + Pandas

- **Decisi贸n final:**  
  Utilizar **Python puro** con **Pandas** como motor de procesamiento de datos.

- **Justificaci贸n t茅cnica:**

  - Pandas es altamente eficiente para datasets de tama帽o peque帽o a medio (hasta varios GB en RAM).
  - El entorno de trabajo requer铆a **velocidad de prototipado** y **simplicidad** en la ejecuci贸n.
  - Amplia experiencia previa trabajando con Pandas, permitiendo aplicar r谩pidamente t茅cnicas de:
    - Limpieza y validaci贸n de datos (`dropna`, filtros condicionales).
    - Transformaciones de columnas (`apply`, `groupby`, `agg`).
    - Escritura y lectura de archivos en formato **Parquet**, aprovechando la eficiencia de almacenamiento de este formato columnar.
  - Uso de librer铆as auxiliares como:
    - `pyarrow` para mejorar el performance al leer y escribir Parquet.
    - `pathlib` para manejo robusto de rutas de archivos.
    - `logging` para trazabilidad y observabilidad del pipeline.

- **Resultado:**  
  Aunque Pandas no ofrece paralelismo distribuido como Spark, para los vol煤menes de datos tratados (menos de 2GB), el rendimiento fue aceptable y permiti贸 mantener el control total del pipeline y su observabilidad.

---

#  Resumen de Tecnolog铆as Usadas

| Herramienta     | Uso                            | Motivo                                        |
| :-------------- | :----------------------------- | :-------------------------------------------- |
| **Python 3.11** | Lenguaje base                  | Versatilidad y soporte de librer铆as modernas. |
| **Pandas**      | ETL (Extract, Transform, Load) | Procesamiento eficiente en memoria.           |
| **PyArrow**     | I/O de archivos Parquet        | Mayor velocidad de serializaci贸n.             |
| **Logging**     | Observabilidad                 | Registro de eventos, errores y tiempos.       |
| **Pathlib**     | Manejo de rutas                | Abstracci贸n multiplataforma de archivos.      |

---

#  Conclusi贸n

Aunque inicialmente se planeaba usar Spark o infraestructura cloud, se prioriz贸 **una soluci贸n funcional, mantenible y eficiente** basada en las herramientas que mejor se adaptaban a las restricciones del proyecto y los limites de mi conocimiento y herramientas.  
El pipeline desarrollado permite escalar relativamente bien a vol煤menes moderados de datos y sirve como una base s贸lida para migraciones futuras a soluciones distribuidas si el tama帽o del dataset o las necesidades de negocio lo requieren.
